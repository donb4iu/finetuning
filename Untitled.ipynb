{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d274fb58-7956-4538-9cb5-60525d3bd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: accelerate in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: datasets in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (0.44.1)\n",
      "Requirement already satisfied: peft in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: trl in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: rich in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: protobuf==4.21.5 in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (4.21.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /home/dbuddenbaum/Documents/cuda/lib/python3.12/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate datasets bitsandbytes peft trl datasets\n",
    "!pip install --upgrade protobuf==4.21.5\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3e978c-df9f-4aa6-8fa3-0ae4cce3a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,TrainingArguments,BitsAndBytesConfig,pipeline,Trainer,DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from peft import LoraConfig,PeftModel,prepare_model_for_kbit_training,get_peft_model,TaskType\n",
    "from trl import SFTTrainer\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3947363e-0cbe-42b8-a1ca-4e0425dc7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aec84ef-f8c1-4b3e-b8bb-67c085105a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'question', 'context'],\n",
       "        num_rows: 78577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c4eb73-6066-452b-bfa4-4cec64a41d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model_id= \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=False)\n",
    "tokenizer= AutoTokenizer.from_pretrained(base_model_id,use_fast=True)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067c620a-ce84-4409-a026-3f7593670027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sample):\n",
    "    start_prompt= \"Write SQL query according to given question or instruction.\\n\\n\"\n",
    "    end_prompt= \"\\n\\nResponse:\"\n",
    "    prompt= [start_prompt+question+end_prompt for question in sample['question']]\n",
    "    sample['input_ids']= tokenizer(prompt,padding='max_length',truncation=True,return_tensors='pt',max_length=512,).input_ids\n",
    "    sample['labels']= tokenizer(sample['answer'],padding='max_length',truncation=True,return_tensors='pt',max_length=512).input_ids\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6305329f-dc03-45d5-a78f-7aa07144d328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea6b1b120ac40579f44c5cc5e7e3232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/78577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset= dataset.map(tokenize_data,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088a6005-9981-4c4c-ad4f-4ef6f85ec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.remove_columns(['answer', 'question', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb631c7-ed41-44e2-9225-1614743b7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db162340-8856-415e-a08b-91ea889e24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144b5c86-c9e7-4607-b0f2-de3e7f0daa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config \n",
    "bnb_config= BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True)\n",
    "\n",
    "# LoRA COnfig\n",
    "peft_config= LoraConfig(lora_alpha=15,\n",
    "                        lora_dropout=0.1,\n",
    "                        target_modules=[\n",
    "                                    'q_proj',\n",
    "                                    'k_proj',\n",
    "                                    'v_proj',\n",
    "                                    'dense'],\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373ef9f1-d1cb-405e-9ac8-a96aa27ff474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819b2a5d517240fd8c10f13fb83afeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model= AutoModelForCausalLM.from_pretrained(base_model_id,quantization_config=bnb_config,device_map=\"auto\")\n",
    "base_model.config.use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006aa771-5bad-4665-aa47-fdcc0c47cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quatnized_model= prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb6ee0bc-f5ad-411b-9c41-3319c0333bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model= get_peft_model(model=quatnized_model,peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56c604f7-3569-49aa-83db-4b519001c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b08fd57-fde9-4a99-92ab-eea764e8d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012fba18-61e8-49d9-8282-385b86a730b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f75f052e24946aea812e5c5d6831bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75acf83b-25d8-4f94-8c7d-41f9decd90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args= TrainingArguments(\n",
    "    output_dir='./working/Mistral-7B-Fine_Tunned',\n",
    "    hub_model_id='dbuddenbaum/Mistral-7B-Fine_Tunned',\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=10)\n",
    "\n",
    "trainer= Trainer(\n",
    "    model= peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e026bf3-785f-4195-9306-daf660e6dfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 24:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.140200</td>\n",
       "      <td>10.109017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=10.111825653076172, metrics={'train_runtime': 1501.9306, 'train_samples_per_second': 0.666, 'train_steps_per_second': 0.083, 'total_flos': 2.1844350468096e+16, 'train_loss': 10.111825653076172, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a760c3af-7ad7-49c0-852e-5b0fab04db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model= \"Mistral-7B-Fine_Tunned1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8ce22b7-3418-40dd-aad4-72ea5e91a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f6be1be-063a-4d74-b7e7-5e08ebbdaa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model= './working/Mistral-7B-Fine_Tunned1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d8257b9-a62e-4ad4-8eec-aac49e0b6973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a98eac99bb42b1904a5e876012996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define the base model ID or path\n",
    "base_model = 'NousResearch/Hermes-2-Pro-Mistral-7B'  # replace with actual path or model ID\n",
    "\n",
    "# Load the base model with appropriate parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             return_dict=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "# Define the path or model ID for the QLoRA adapters\n",
    "lora_adapters = './Mistral-7B-Fine_Tunned1'  # replace with actual path or model ID\n",
    "\n",
    "# Load the QLoRA adapters into the model\n",
    "model = PeftModel.from_pretrained(model, lora_adapters)\n",
    "\n",
    "# Merge the QLoRA adapters into the base model and unload adapters\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Load the tokenizer for the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side='right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd4de730-f454-4e5d-beb9-0d1a78af17b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5423c9fdb134a5296ae28a018db2aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef90005c219941e0813e0533c20ff172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308a91dc34274bdfb6507daec4ea01ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60dffa71edd406e9c74b0fb264c21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/9.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dbuddenbaum/Mistral-7B-Fine_Tunned_Model/commit/f84aae339061d0c5dab4e8f61d5163eb6129b750', commit_message='Upload MistralForCausalLM', commit_description='', oid='f84aae339061d0c5dab4e8f61d5163eb6129b750', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dbuddenbaum/Mistral-7B-Fine_Tunned_Model', endpoint='https://huggingface.co', repo_type='model', repo_id='dbuddenbaum/Mistral-7B-Fine_Tunned_Model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Mistral-7B-Fine_Tunned_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ba0fb4b-09f6-4406-9318-c358053634dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05a0c7287904a5ca5a9ed461ba9464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d10e8e8c6184fdb896718cc4b99ceda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dbuddenbaum/Mistral-7B-Fine_Tunned_Model/commit/e81f43e67e5a6101a95bf93e6cff8f004b4cbdf1', commit_message='Upload tokenizer', commit_description='', oid='e81f43e67e5a6101a95bf93e6cff8f004b4cbdf1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/dbuddenbaum/Mistral-7B-Fine_Tunned_Model', endpoint='https://huggingface.co', repo_type='model', repo_id='dbuddenbaum/Mistral-7B-Fine_Tunned_Model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"Mistral-7B-Fine_Tunned_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bf0d6da-8a57-4fa0-b097-0e5e1d140474",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll= '''You have a database with two tables: employees and departments. The employees table contains information about each employee, and the departments table contains information about each department.Write an SQL query to retrieve the names of all employees along with the name of the department they belong to.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e36d77a6-9083-460d-a409-398db4127172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write SQL query according to given question or instruction.\n",
      "\n",
      "You have a database with two tables: employees and departments. The employees table contains information about each employee, and the departments table contains information about each department.Write an SQL query to retrieve the names of all employees along with the name of the department they belong to.\n",
      "\n",
      "Response:\n",
      "SELECT employees.name, departments.name AS department\n",
      "FROM employees\n",
      "JOIN departments ON employees.department_id = departments.id;\n",
      "\n",
      "The SQL query above uses a JOIN statement to combine the employees and departments tables based on the department_id column in the employees table and the id column in the departments table. The SELECT statement then retrieves the name column from both tables, using the AS keyword to alias the departments.name column as department. This will give us the names of all employees and the names of the departments they belong to in the result set.\n",
      "\n",
      "If you have any further questions\n"
     ]
    }
   ],
   "source": [
    "start_prompt= \"Write SQL query according to given question or instruction.\\n\\n\"\n",
    "end_prompt= \"\\n\\nResponse:\"\n",
    "prompt= start_prompt+ll+end_prompt\n",
    "\n",
    "pipe= pipeline(task='text-generation',model=model,tokenizer=tokenizer,max_length=200)\n",
    "result= pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a2d45-9bc3-4d9d-a652-3d616cff337c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
